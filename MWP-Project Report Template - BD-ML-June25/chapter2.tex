\chapter{Literature Review}

In this chapter, we discuss the previous research on blockchain-based federated learning for various domains.  Here, we also review the work that has previously been done in the field of federated learning and trustworthy AI.

\section{Blockchain-Based Federated Learning}

 The paper \cite{chaves2024federated} addresses key challenges in Federated Learning (FL) that hinder its widespread adoption, despite its benefits for data privacy in machine learning. Specifically, it focuses on the difficulties in guaranteeing traceability and openness in the FL process, and the lack of effective mechanisms for incentivizing participant contributions. Traditional FL frameworks often struggle with maintaining an immutable audit trail of training activities, ensuring transparency in how contributions are made and models are aggregated, and fairly rewarding participants, which can impede the overall robustness and trustworthiness of collaborative model training, especially when dealing with dynamic data streams. To tackle these problems, the research proposes the integration of Ethereum blockchain technology with the Kafka-ML framework, an open-source platform designed for machine learning with data streams. This novel approach introduces an asynchronous, blockchain-based Federated Learning system. Ethereum's immutable ledger is utilized for transparent and auditable tracking of all participant activities and model updates. Furthermore, Ethereum smart contracts are employed to automate reward distribution systems, aiming to create equitable incentive structures that encourage active and honest participation in the FL process. The architecture involves modifications to the Kafka-ML Platform and Kafka-ML Federated modules to interact with a private Ethereum node, where smart contracts manage the queue of models for aggregation and log all FL-related transactions. 
 
 Moreover, this paper presents an evaluation of this blockchain-integrated Kafka-ML framework using a use case of wireless network technology detection with IQ samples. Experiments were conducted with 2, 4, and 8 federated clients, comparing the blockchain-based FL approach with a classic FL implementation within Kafka-ML. In terms of accuracy, when clients had the full dataset, both implementations achieved similar validation accuracies, generally stabilizing around 88\% after several aggregation rounds. For instance, with 2 clients, both approaches reached this level, with slight variations in convergence speed. When the dataset was randomly split, accuracies were slightly lower for the blockchain method in some configurations but remained comparable (e.g., around 87--89\% for 2 clients). Training times were slightly higher for the blockchain implementation with the full dataset due to transaction confirmation overhead (e.g., for 2 clients, Blockchain FL at $\sim$2100s and Classic FL at $\sim$2150s for 2 clients;  Blockchain FL at $\sim$2050s and Classic FL at $\sim$1750s for 2 clients with split data, indicating blockchain was slower here). However, with split datasets, the blockchain approach was sometimes faster, potentially due to avoiding bottlenecks in Kafka control messages when datasets are smaller and training phases complete earlier. The system also demonstrated a token-based reward distribution, although some imbalances were observed, particularly as the number of clients increased or when initial contributions significantly outperformed later ones. 
 
 In addition, the paper identifies several areas for future work and potential improvements. One key area is the integration of new model aggregation techniques beyond FedAvg, such as ASO-Fed, FedDR, or AsyncFedED, to better handle non-uniformly distributed data or scenarios with significantly slower clients, which could otherwise lead to skewed models. Another suggested improvement is refining the reward distribution system to ensure fairer compensation, as the proof-of-concept showed some imbalances. The authors also propose integrating data processing tasks directly within Kafka-ML to be applied to data streams before or after model interaction. Furthermore, adding automatic hyperparameter optimization is considered a valuable future functionality. Lastly, exploring the InterPlanetary File System (IPFS) as an alternative decentralized mechanism for model sharing is suggested to potentially enhance availability, robustness, and data integrity compared to relying solely on Apache Kafka for centralized data transfer.


\section{Communication-Efficient Federated Learning}
%\section{Communication-Efficient Learning of Deep Networks
%from Decentralized Data}

%\section*{Problem Addressed}
The core challenge identified is leveraging the vast amounts of data available on modern mobile devices for training machine learning models, which can significantly enhance the user experience. However, this data is often privacy-sensitive, voluminous, or both, making its transfer to centralized data centers for traditional model training problematic. The paper \cite{mcmahan2017communication}  focuses on developing a method to learn a shared model from this decentralized data without requiring the data to leave the user's device. This setting, termed ``federated optimization,'' presents unique characteristics that differentiate it from typical distributed optimization problems, including: 1) Non-IID data, where each client's local dataset is not representative of the overall population distribution; 2) Unbalanced data, with varying amounts of training data across different users; 3) Massively distributed nature, with a very large number of participating clients; and 4) Limited communication, as mobile devices frequently experience slow, expensive, or intermittent network connections. Among these, communication cost is highlighted as the principal constraint.

The author advocates for an approach called "Federated Learning", where the training data remains distributed on mobile devices, and a shared global model is learned by aggregating locally-computed updates from these devices. To implement this, the authors introduce the "Federated Averaging (FedAvg)" algorithm. In this algorithm, a central server coordinates the learning process. In each communication round, the server selects a fraction, \(C\), of the available clients. These selected clients download the current global model parameters from the server. Each client then performs local computation by updating the model based on its local dataset, typically by running stochastic gradient descent (SGD) for a certain number of local epochs, \(E\), using a local minibatch size, \(B\). After local training, clients send their updated model parameters (or the changes to the parameters) back to the server. The server then aggregates these updates, commonly by performing a weighted average of the client models, to produce an improved global model for the next round. The primary goal of FedAvg is to reduce the number of communication rounds required to train a high-quality model by performing more computation locally on each client.

The FedAvg algorithm was extensively evaluated across five different model architectures and four datasets, demonstrating its practicality and robustness, particularly for the unbalanced and non-IID data distributions characteristic of the federated setting. A key finding was that FedAvg significantly reduces the number of communication rounds needed for training, typically by 10-100x compared to a baseline FederatedSGD (FedSGD) approach where clients compute a single batch gradient per round. For instance, in training a CNN on MNIST data (IID partition), FedAvg achieved 99\% test accuracy in 18 communication rounds (with parameters $E=10, B=10$), a 34.8x speedup over FedSGD which required 626 rounds. For a pathologically non-IID partition of MNIST, FedAvg still provided a 2.8x speedup. On a more realistic, unbalanced, and non-IID Shakespeare dataset for language modeling, FedAvg showed a remarkable 95.3x speedup in reaching the target accuracy (41 rounds for FedAvg vs. 3906 for FedSGD). Furthermore, FedAvg often converged to higher test-set accuracy levels than the baseline FedSGD models; for example, the MNIST CNN reached 99.44\% accuracy with FedAvg, compared to 99.22\% with FedSGD. On a large-scale LSTM next-word prediction task, FedAvg achieved a 23x reduction in communication rounds to reach the target accuracy compared to FedSGD (35 rounds vs. 820 rounds).

The work notes that for some models, particularly in the later stages of convergence, performing an excessive number of local epochs (a large \(E\)) between averaging steps can lead to the FedAvg algorithm plateauing or even diverging. This suggests that, similar to decaying learning rates, it might be beneficial to decay the amount of local computation per round as training progresses. The study primarily focused on a controlled experimental environment with a fixed set of clients and synchronous updates. However, a practical deployment of federated learning must address more complex real-world scenarios, such as fluctuating client availability (which may correlate with local data distributions), changes in client datasets over time, and handling clients that become unresponsive or send corrupted updates; these aspects were identified as beyond the scope of the work presented. While Federated Learning offers significant practical privacy advantages by keeping raw data on devices, the paper suggests that future work could explore providing stronger, more formal privacy guarantees through techniques like differential privacy and secure multi-party computation. The authors also mention that subsequent research by others has already begun to address some of these areas, such as developing efficient secure aggregation protocols and further strategies for improving communication efficiency.


The work \cite{konevcny2016federated} addresses the critical challenge of communication efficiency in the Federated Learning (FL) setting. In FL, a centralized model is trained using data that remains distributed across a large number of clients, often mobile phones, which typically have unreliable and slow network connections, particularly for uplink communication. The naive approach requires each client to send a full model update to the server in each round, which becomes a bottleneck for large models due to asymmetric internet speeds (slower uplink) and increased data size from cryptographic protocols for privacy. Therefore, the primary problem is to reduce the uplink communication costs without significantly compromising the performance of the learned model. The authors propose two primary strategies to reduce uplink communication costs: structured updates and sketched updates. Structured updates involve directly learning an update from a restricted space, parametrized by fewer variables. Two types of structures are explored: low-rank updates, where the update matrix $H_t^i$ is forced to be a product of two smaller matrices $A_t^i B_t^i$ (with $A_t^i$ often fixed and randomly generated), and random mask updates, where the update $H_t^i$ is restricted to a predefined random sparsity pattern, with only non-zero entries and a seed for the pattern being transmitted. Sketched updates involve learning a full model update first and then compressing it before sending it to the server. This is achieved through techniques such as subsampling (sending only a random subset of values), probabilistic quantization (e.g., quantizing each scalar to one or more bits in an unbiased way), and improving quantization by applying structured random rotations (using Walsh-Hadamard and binary diagonal matrices) before quantization to make scales more uniform. 

The proposed methods demonstrated significant improvements in communication efficiency. Experiments showed that these techniques can reduce the communication cost by up to two orders of magnitude with only a slight degradation in convergence speed or model accuracy. For instance, on the CIFAR-10 dataset, using an all-convolutional model, good prediction accuracy was achieved while communicating less total information than the size of the original CIFAR data. Specifically, by preprocessing with random rotation, sketching out all but 6.25\% of the update elements, and using 2-bit quantization, a 256-fold reduction in bits needed to represent layer updates was achieved with only a minor drop in convergence. In a more realistic scenario using Reddit data for next-word prediction with an LSTM, the randomized Hadamard transform with 2-bit quantization incurred no loss in performance, and the model could be trained efficiently even before using the data of every user once. The random mask structured updates were found to perform significantly better than low-rank updates as the size of the updates was reduced. 

While the paper presents effective strategies, it also points to areas for further exploration and acknowledges certain limitations. The authors explicitly state that they did not experiment with combining structured updates and sketched updates in their work, suggesting this as a potential avenue for future improvement. Although structured random mask updates eventually converged to slightly higher accuracy, sketched updates were observed to attain modest accuracy faster in some CIFAR experiments. The paper notes that the slight decrease in accuracy with sketching is theoretically expected due to increased variance in updates. Furthermore, the authors mention that while the metrics used (like minimizing upload size or number of rounds) are useful proxies, they may not be the direct objectives in a practical large-scale deployment of Federated Learning, given the current lack of extensive experience with inherent real-world issues. The paper also highlights a practical trade-off: using more clients per round, each communicating less (e.g., through more aggressive subsampling), can achieve similar accuracy to using fewer clients that communicate more, which is an important consideration for varying network conditions and client availability. 


\section{Federated Learning in Smart Buildings}

The paper \cite{khan2022occupancy} addresses the significant energy consumption in buildings, which constitutes a large portion of worldwide energy expenditure. A key strategy to mitigate this is by understanding and predicting building occupancy to avoid energy waste, thereby saving costs and reducing CO2 emissions. Specifically, the paper aims to tackle the challenge of providing accurate occupancy predictions for spaces within a building where specific model training has not been performed, often due to a lack of historical operational data, sensor failures, or irregular data collection. While transfer learning can help, traditional methods raise privacy and data security concerns when data from different buildings or rooms is shared. This paper proposes an approach that leverages Federated Learning (FL) and Long Short-Term Memory (LSTM) neural networks for occupancy prediction across several rooms in a building. The core idea is to use FL to collaboratively train a shared occupancy prediction model using data from multiple rooms (edge nodes) without directly sharing their local datasets, thus preserving data privacy. The architecture consists of three layers: an IoT Layer for collecting sensor data (e.g., temperature, humidity, CO2, light); an Edge Layer where Edge Computing Nodes (ECNs) host and train local LSTM models using data from their respective environments; and a Cloud/Edge Aggregator Layer where an Aggregator Node (AN) merges the models from the ECNs and disseminates the updated global model back to them. This allows rooms that cannot perform local training (e.g., due to missing sensors for ground truth occupancy) to receive and utilize the globally trained model.

The effectiveness of the proposed FL-LSTM approach was demonstrated through simulation experiments using a publicly available dataset. In a setup simulating eleven rooms, where ten rooms participated in FL training and the eleventh room used the aggregated model without prior local training, the system achieved high accuracy. For the test set (representing the eleventh room), the occupancy prediction model achieved an accuracy of approximately 0.945. Further evaluation metrics, derived from the confusion matrix for this test set, included a precision of 0.925, a recall of 0.915, and an F1 score of 0.920. These results indicate that the approach is promising for providing a preliminary, yet accurate, occupancy model for all building rooms, including those without sufficient local data for individual model training.

The paper acknowledges that the current simulations were conducted using an existing, publicly available dataset. As part of future work, the authors intend to create their extended dataset, which will include heterogeneous sensor data collected from different rooms within their research institute. They plan to then use this new dataset to further test and refine their occupancy prediction models. Additionally, the research aims to enhance the prediction capabilities beyond simple occupancy (occupied/unoccupied) to estimate the approximate number of people in a room. Another direction for future improvement is to understand and predict not just occupancy, but also specific activities occurring within the building environments.


\section{Trustworthy Federated Learning}

The fundamental problem addressed in this paper \cite{lo2022toward} is the "black-box" nature of many sophisticated Artificial Intelligence (AI) models, particularly Deep Neural Networks (DNNs). While these models achieve remarkable performance in various tasks, their decision-making processes are often opaque and challenging for humans to comprehend and trust. This lack of transparency hinders the adoption of AI in critical sectors like healthcare, finance, and law enforcement, where understanding the reasoning behind a decision is essential for accountability, safety, and ethical considerations. Consequently, there's a pressing need for Explainable AI (XAI) methods to improve trust, interpretability, and the ability to verify that AI systems are making fair, unbiased, and fact-based decisions. The paper notes that while existing surveys have covered XAI concepts and post-hoc methods, there's a gap in reviewing assessment methods, tools, datasets, and other related aspects systematically.

This paper addresses the problem by providing a comprehensive and structured overview of the XAI landscape, aiming to guide researchers and practitioners toward developing more trustworthy AI. The authors conducted an extensive literature review, analyzing 410 critical articles published between January 2016 and October 2022. They propose a hierarchical categorization system for XAI techniques, dividing them into four main axes: (i) data explainability (understanding and preparing data), (ii) model explainability (creating inherently interpretable models or making complex models more understandable), (iii) post-hoc explainability (explaining model decisions after they are made), and (iv) the assessment of explanations. The paper delves into each axis, discussing various techniques, available evaluation metrics, open-source packages, and datasets. It also outlines XAI concerns from legal, user, and application perspectives, advocating for tailoring explanation content to specific user types.

As a comprehensive survey, the primary outcome of this paper is not a specific accuracy or error rate from a novel algorithm but rather a structured synthesis of knowledge in the XAI field and a framework for advancing it. The authors successfully categorize and review a vast body of literature, identifying current research trends and practices. Key contributions include the proposed four-axes methodology for developing and evaluating XAI, which encourages a holistic approach to explainability throughout the AI development pipeline. The paper also provides a detailed discussion on the assessment of explanations, including cognitive psychological measures, understandability, satisfaction, trust, transparency, and computational assessment, which is often overlooked in other reviews. Furthermore, it compiles information on available XAI tools, open-source datasets, and highlights the importance of considering different stakeholder perspectives (regulatory, user, application). The systematic literature search strategy itself is an outcome, resulting in the identification of 410 key papers and their categorization (e.g., 56 papers on data explainability, 67 on model explainability, 126 on post-hoc explainability, and 58 on explainability assessment).

The paper itself, being a survey, focuses on identifying gaps and future directions within the XAI field rather than having inherent drawbacks in its own methodology that require improvement. It explicitly highlights several open challenges and areas for future research in XAI, which can be interpreted as current limitations or areas needing advancement in the broader field. These include the need for better XAI system design, improved generalization of XAI methods across different models and domains, more effective user interactions with XAI systems, robust XAI ground truth evaluation methodologies, and the development of more advanced XAI tools. The paper points out that despite many available methods, there isn't a consensus on which methodology delivers the best explanations or how to definitively assess their quality. It also notes that explainability alone is insufficient for achieving trustworthy AI; it must be coupled with robustness, causality studies, data governance, security, and accountability. A critical future direction emphasized is the need to bridge the gap between legal/regulatory requirements for trustworthy AI and the technical advancements and tools in XAI, fostering collaboration between legal and technical experts. The challenge of explaining modern generative models like GPT-3 or DALL-E 2 is also cited as an unexplored and complex arena.


The primary problem addressed in this paper \cite{ali2023explainable} is the lack of trustworthiness in federated learning systems, specifically focusing on the challenges of accountability and fairness. Federated learning systems, while preserving privacy by training models locally without transferring raw data externally, struggle with these issues due to the involvement of multiple stakeholders and heterogeneity in client data distributions. The paper notes that conventional federated learning systems do not adequately track or map the local data and models to the global models, making it difficult to ascertain responsibility if the model underperforms or to ensure fair treatment across different data groups. This is particularly problematic in scenarios like medical diagnostics where biased or unaccountable models can have significant consequences.


To address these challenges, the paper proposes a blockchain-based trustworthy federated learning architecture. Accountability is tackled through the design of a smart contract-based data-model provenance registry. This registry immutably records hashed versions of local data used for training, local model versions, and global model versions on the blockchain, enabling auditing and traceability without exposing raw data. For fairness, the paper introduces a weighted fair training data sampler algorithm. This algorithm aims to mitigate bias caused by heterogeneous data distributions (Non-IID data) by dynamically sampling training data from underrepresented classes based on the inverse of the weight distribution of a common test dataset, thus promoting more balanced local model training.


The proposed solution was evaluated using a COVID-19 X-ray detection use case. The results demonstrated the feasibility of the architecture in enabling accountability and improving fairness. Specifically, the weighted fair data sampler algorithm showed better performance compared to the default federated learning setup in terms of model generalization and accuracy. For instance, models (ResNet50 and GhostNet) trained with weighted fair sampled data generally achieved lower training losses and higher training and test accuracies across multiple experiments. The GhostNet model with fair sampling, for example, showed a test accuracy of up to 85.30\%, compared to 85.22\% without it in one experimental group (Group 3), and also demonstrated improved fairness in predictions by reducing bias towards majority classes in the confusion matrices. Furthermore, the performance of blockchain operations, such as uploading hashed model parameters and data versions, showed acceptable latency (e.g., average latency for parallel upload was 3420.14 ms, and model retrieval was 4.32 ms), supporting the feasibility of its integration for an accountable environment.


The paper acknowledges certain limitations and areas for future improvement. The current work is confined to addressing only the fairness and accountability aspects of trustworthy AI. Other dimensions of trustworthiness were not explored in this study. As for future enhancements, the authors plan to explore the integration of incentive mechanisms within the federated learning system using blockchain and smart contracts. Such mechanisms could further encourage honest participation and contribute to overall system trustworthiness and fairness.



\section{Summary}

%\section{Inferences from Literature Survey}
\label{sec:lit_survey_inferences}
Following the literature review, the summary is provided in Table 2.1, and from there, some inferences have been outlined as follows:  
%The comprehensive literature survey provides a strong foundation for the proposed project, directly supporting its core objectives:

\begin{enumerate}
    \item \textbf{Validation of FL-Blockchain Integration for Trustworthy Systems:} The literature confirms the viability of integrating Federated Learning (FL) with Blockchain technology to create more trustworthy and accountable AI systems. Specifically, works like in \cite{chaves2024federated}  demonstrate the use of Kafka-ML in such integrations with a focus on reliability and data streams, while the work by Lo et al. \cite{lo2022toward} explicitly addresses accountability and fairness through blockchain mechanisms. This directly underpins the project's objective to develop a fair and secure FL framework using Kafka-ML and blockchain.

    \item \textbf{Established Application of FL in Occupancy Prediction and Fairness Considerations:} The successful application of Federated Learning for building occupancy prediction is established in the literature, as seen in \cite{khan2022occupancy}. This validates the chosen application domain. Furthermore, the challenge of ensuring fairness in such distributed systems is addressed by research focusing on blockchain-based solutions and specialized samplers to mitigate bias, aligning with the project's goal of incorporating fairness into the occupancy prediction model.

    \item \textbf{Support for Real-Time Data Handling and System Efficiency:} The surveyed literature acknowledges the importance of handling real-time data streams in FL systems and addresses the critical aspect of communication efficiency in distributed learning environments. These findings are crucial for the project's aim to utilize real-time sensor data for occupancy prediction and to build an efficient FL system.
\end{enumerate}

%\begin{longtable}
\begin{sidewaystable}
\small
\centering
\caption{Literature Survey Summary}
\label{tab:lit_survey_summary} 
\begin{tabular}{|@{}p{0.22\textwidth} |c|c| c| c| c| c| c| c| c| c| c| c|@{}} % Adjusted first column width
    \toprule
    \textbf{Paper Title} &
    \rot{Publication Year} &
    \rot{Federated Learning (FL)} &
    \rot{Blockchain} &
    \rot{Trustworthiness As focus} &
    \rot{XAI / Explainability} &
    \rot{Accountability via Blockchain} &
    \rot{Fairness via Blockchain / Sampler} &
    \rot{Communication Efficiency} &
    \rot{Real Time Data Streams} &
    \rot{Smart Contracts} &
    \rot{FedAvg Algorithm} &
    \rot{Application: Building Occupancy} \\
    \midrule
   
     
    % --- Data from Slide 6 ---
    Federated Learning Meets Blockchain - A Kafka-
ML Integration for reliable model training using
data streams &
    2023/2024 & x & x & x &   & x &   & x & x &   &   &   \\
    \midrule
    Occupancy Prediction in Buildings: An approach
leveraging LSTM and Federated Learning &
    2022 & x &   &   &   &   &   &   &   &   &   & x \\
    \midrule
    Toward Trustworthy AI: Blockchain-Based Ar-
chitecture Design for Accountability and Fair-
ness of Federated Learning Systems &
    2022 & x & x & x &   & x & x &   & x &   &   &   \\
    \midrule
    Communication-Efficient Learning of Deep Net-
works from Decentralized Data &
    2016 & x &   &   &   &   &   & x &   &   &   &   \\
    \midrule
    Federated Learning: Strategies for Improving Com-
munication Efficiency &
    2017 & x &   &   &   &   &   & x &   & x &   &   \\
    \midrule
    Explainable Artificial Intelligence (XAI): What
We Know and What is Left to Attain Trustwor-
thy Artificial Intelligence &
    2023 &   &   & x & x &   &   &   &   &   &   &   \\
    % --- End of data ---
    \toprule
\end{tabular}

\end{sidewaystable}





