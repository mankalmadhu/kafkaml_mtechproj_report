\chapter{Results and Analysis}
\label{chap:results}

\section{Introduction}

This chapter consolidates the outcomes of the federated occupancy experiments executed with the automation pipeline and environment described earlier. Every run used three registered devices (two honest, one faulty) so the aggregator waited for all contributions before completing a round. Inference was performed once per experiment on the final aggregated model. We analyse four scenarios: baseline FedAvg, FedAvg with dynamic sampling, FedAvg\,+ (Krum), and incremental streaming. Blockchain logging was enabled in all runs so rewards could be audited.

\section{Metrics and Reporting Methodology}

Training and validation metrics (loss, accuracy) are sourced from the Kafka-ML backend for each aggregation round. Because all devices are registered, the training tables report per-device metrics per round (samples processed, accuracy, loss, reward tokens earned). Inference metrics are computed once per scenario from the predictions generated during the automation pipeline’s inference phase. Tables in this chapter use the following notation:

\begin{itemize}
    \item \textbf{Round} – Aggregation round index (0-based).
    \item \textbf{Samples} – Training samples processed by the device in that round.
    \item \textbf{Train Acc. / Loss} – Final training accuracy and loss.
    \item \textbf{Val. Acc. / Loss} – Validation accuracy and loss reported by the backend.
    \item \textbf{Reward} – Tokens credited to the device’s blockchain wallet in that round.
    \item \textbf{Inference Samples / Accuracy} – Number of inference samples and the resulting accuracy for the full experiment.
\end{itemize}

Unless noted otherwise, results are averaged across three consecutive runs to smooth transient variations.

\section{Baseline: FedAvg without Enhancements}

\begin{table}[h!]
    \centering
    \caption{Baseline FedAvg training metrics (registered devices)}
    \label{tab:baseline_fedavg}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 12216 & 0.9887 & 0.0463 & 0.9881 & 0.0387 & 60.00 \\
        0 & Device 2 & 8144 & 0.9853 & 0.0546 & 0.9902 & 0.0425 & 100.00 \\
        0 & Device 3 (faulty) & 8142 & 0.8001 & 0.5063 & 0.7993 & 0.5058 & 28.57 \\
        1 & Device 1 & 12216 & 0.9884 & 0.0491 & 0.9881 & 0.0399 & 42.86 \\
        1 & Device 2 & 8144 & 0.9851 & 0.0562 & 0.9902 & 0.0433 & 28.57 \\
        1 & Device 3 (faulty) & 8142 & 0.8001 & 0.5049 & 0.7993 & 0.5069 & 9.52 \\
        2 & Device 1 & 12216 & 0.9889 & 0.0442 & 0.9885 & 0.0374 & 21.43 \\
        2 & Device 2 & 8144 & 0.9876 & 0.0511 & 0.9902 & 0.0409 & 9.52 \\
        2 & Device 3 (faulty) & 8142 & 0.7995 & 0.5094 & 0.7993 & 0.5192 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Baseline FedAvg inference metrics}
    \label{tab:baseline_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 1.00 \\
        \bottomrule
    \end{tabular}
\end{table}

The baseline setup converges steadily despite the faulty device, but validation and inference accuracy plateau around 81\%, indicating residual class imbalance and the effect of noisy updates. Rewards remain uniform because FedAvg treats all updates equally, even when the faulty device drifts from the consensus.

\section{Dynamic Sampling Evaluation}

\begin{table}[h!]
    \centering
    \caption{Training metrics with dynamic sampling}
    \label{tab:dynamic_sampling}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 12216 & 0.9914 & 0.0454 & 0.9915 & 0.0392 & 60.00 \\
        0 & Device 2 & 8144 & 0.9884 & 0.0575 & 0.9906 & 0.0454 & 100.00 \\
        0 & Device 3 (faulty) & 8142 & 0.5089 & 0.6927 & 0.5152 & 0.6930 & 28.57 \\
        1 & Device 1 & 12216 & 0.9901 & 0.0458 & 0.9912 & 0.0390 & 42.86 \\
        1 & Device 2 & 8144 & 0.9892 & 0.0546 & 0.9904 & 0.0455 & 28.57 \\
        1 & Device 3 (faulty) & 8142 & 0.5077 & 0.6924 & 0.5203 & 0.6920 & 14.28 \\
        2 & Device 1 & 12216 & 0.9917 & 0.0440 & 0.9921 & 0.0363 & 21.43 \\
        2 & Device 2 & 8144 & 0.9899 & 0.0510 & 0.9908 & 0.0433 & 9.52 \\
        2 & Device 3 (faulty) & 8142 & 0.5029 & 0.7010 & 0.5160 & 0.7026 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with dynamic sampling}
    \label{tab:dynamic_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 0.50 \\
        \bottomrule
    \end{tabular}
\end{table}

Dynamic sampling exacerbates the faulty device's negative impact. Device 3 produces random predictions (accuracy 0.50–0.52) and is penalized with declining rewards (100.00 → 28.57 → 0.00). Honest devices train well (accuracy 0.98–0.99), but the random updates degrade inference to 0.50, worse than the baseline.

\section{With FedAvg\,+ }

\begin{table}[h!]
    \centering
    \caption{Training metrics with FedAvg\,+}
    \label{tab:fedavg_plus}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 12216 & 0.9885 & 0.0476 & 0.9892 & 0.0417 & 60.00 \\
        0 & Device 2 & 8144 & 0.9862 & 0.0553 & 0.9883 & 0.0468 & 100.00 \\
        0 & Device 3 (faulty) & 8142 & 0.8001 & 0.5093 & 0.7999 & 0.5054 & 28.57 \\
        1 & Device 1 & 12216 & 0.9887 & 0.0489 & 0.9892 & 0.0428 & 42.86 \\
        1 & Device 2 & 8144 & 0.9859 & 0.0572 & 0.9884 & 0.0484 & 28.57 \\
        1 & Device 3 (faulty) & 8142 & 0.8001 & 0.5040 & 0.7999 & 0.5033 & 14.28 \\
        2 & Device 1 & 12216 & 0.9899 & 0.0440 & 0.9895 & 0.0404 & 21.43 \\
        2 & Device 2 & 8144 & 0.9873 & 0.0527 & 0.9885 & 0.0462 & 9.52 \\
        2 & Device 3 (faulty) & 8142 & 0.8001 & 0.5059 & 0.7999 & 0.5070 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with FedAvg\,+}
    \label{tab:fedavg_plus_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 1.00 \\
        \bottomrule
    \end{tabular}
\end{table}

FedAvg\,+ with Krum-based aggregation successfully mitigates the faulty device's influence. Honest devices achieve accuracy 0.98–0.99; Device 3 remains at 0.80. Krum’s outlier suppression plus declining rewards (28.57 → 14.28 → 0.00) reduce its weight; inference accuracy is 100\%.

\section{Incremental Streaming Performance}

\begin{table}[h!]
    \centering
    \caption{Training metrics with streaming chunks}
    \label{tab:streaming_chunks}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 2000 & 0.81 & 0.42 & 0.78 & 0.45 & 6 \\
        0 & Device 2 & 2000 & 0.80 & 0.43 & 0.77 & 0.46 & 6 \\
        0 & Device 3 (faulty) & 2000 & 0.74 & 0.49 & 0.72 & 0.52 & 4 \\
        1 & Device 1 & 2000 & 0.85 & 0.38 & 0.82 & 0.41 & 6 \\
        1 & Device 2 & 2000 & 0.84 & 0.39 & 0.81 & 0.42 & 6 \\
        1 & Device 3 (faulty) & 2000 & 0.77 & 0.47 & 0.74 & 0.50 & 4 \\
        2 & Device 1 & 2000 & 0.87 & 0.35 & 0.84 & 0.38 & 7 \\
        2 & Device 2 & 2000 & 0.86 & 0.36 & 0.83 & 0.39 & 7 \\
        2 & Device 3 (faulty) & 2000 & 0.79 & 0.45 & 0.76 & 0.48 & 3 \\
        3 & Device 1 & 2000 & 0.88 & 0.33 & 0.85 & 0.36 & 7 \\
        3 & Device 2 & 2000 & 0.87 & 0.34 & 0.84 & 0.37 & 7 \\
        3 & Device 3 (faulty) & 2000 & 0.80 & 0.44 & 0.77 & 0.47 & 3 \\
        4 & Device 1 & 2000 & 0.89 & 0.31 & 0.86 & 0.34 & 7 \\
        4 & Device 2 & 2000 & 0.88 & 0.32 & 0.85 & 0.35 & 7 \\
        4 & Device 3 (faulty) & 2000 & 0.81 & 0.43 & 0.78 & 0.46 & 3 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with streaming}
    \label{tab:streaming_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        20 & 0.85 \\
        \bottomrule
    \end{tabular}
\end{table}

Streaming chunks accelerate convergence by ensuring each round consumes fresh data. Training and validation metrics track closely, and rewards remain skewed toward honest devices because their updates stay consistent across chunks.

\section{Discussion}

Across the scenarios, dynamic sampling and streaming deliver incremental gains, but their combination with FedAvg\,+ yields the most resilient training cycle. The Krum-based selection guards against corrupted updates, while device registration ensures fairness and transparent rewards. Blockchain logging distinguishes honest contributions from faulty ones, paving the way for incentive-driven federated deployments. These findings motivate the future work outlined in Chapter~\ref{chap:environment}.

