\chapter{Results and Analysis}
\label{chap:results}

\section{Introduction}

This chapter consolidates the outcomes of the federated occupancy experiments executed with the automation pipeline and environment described earlier. Every run used three registered devices (two honest, one faulty) so the aggregator waited for all contributions before completing a round. Inference was performed once per experiment on the final aggregated model. We analyse four scenarios: baseline FedAvg, FedAvg with dynamic sampling, FedAvg\,+ (Krum), and incremental streaming. Blockchain logging was enabled in all runs so rewards could be audited.

\section{Metrics and Reporting Methodology}

Training and validation metrics (loss, accuracy) are sourced from the Kafka-ML backend for each aggregation round. Because all devices are registered, the training tables report per-device metrics per round (samples processed, accuracy, loss, reward tokens earned). Inference metrics are computed once per scenario from the predictions generated during the automation pipeline’s inference phase. Tables in this chapter use the following notation:

\begin{itemize}
    \item \textbf{Round} – Aggregation round index (0-based).
    \item \textbf{Samples} – Training samples processed by the device in that round.
    \item \textbf{Train Acc. / Loss} – Final training accuracy and loss.
    \item \textbf{Val. Acc. / Loss} – Validation accuracy and loss reported by the backend.
    \item \textbf{Reward} – Tokens credited to the device’s blockchain wallet in that round.
    \item \textbf{Inference Samples / Accuracy} – Number of inference samples and the resulting accuracy for the full experiment.
\end{itemize}

Unless noted otherwise, results are averaged across three consecutive runs to smooth transient variations.

\section{Baseline: FedAvg without Enhancements}

\begin{table}[h!]
    \centering
    \caption{Baseline FedAvg training metrics (registered devices)}
    \label{tab:baseline_fedavg}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 3000 & 0.80 & 0.45 & 0.76 & 0.48 & 5 \\
        0 & Device 2 & 3000 & 0.79 & 0.46 & 0.75 & 0.49 & 5 \\
        0 & Device 3 (faulty) & 3000 & 0.72 & 0.55 & 0.70 & 0.57 & 5 \\
        1 & Device 1 & 3000 & 0.83 & 0.42 & 0.78 & 0.46 & 5 \\
        1 & Device 2 & 3000 & 0.82 & 0.43 & 0.77 & 0.47 & 5 \\
        1 & Device 3 (faulty) & 3000 & 0.74 & 0.53 & 0.71 & 0.55 & 5 \\
        2 & Device 1 & 3000 & 0.85 & 0.39 & 0.80 & 0.44 & 5 \\
        2 & Device 2 & 3000 & 0.84 & 0.40 & 0.79 & 0.45 & 5 \\
        2 & Device 3 (faulty) & 3000 & 0.76 & 0.51 & 0.73 & 0.53 & 5 \\
        3 & Device 1 & 3000 & 0.86 & 0.37 & 0.81 & 0.42 & 5 \\
        3 & Device 2 & 3000 & 0.85 & 0.38 & 0.80 & 0.43 & 5 \\
        3 & Device 3 (faulty) & 3000 & 0.77 & 0.50 & 0.74 & 0.52 & 5 \\
        4 & Device 1 & 3000 & 0.87 & 0.35 & 0.82 & 0.40 & 5 \\
        4 & Device 2 & 3000 & 0.86 & 0.36 & 0.81 & 0.41 & 5 \\
        4 & Device 3 (faulty) & 3000 & 0.78 & 0.49 & 0.75 & 0.51 & 5 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Baseline FedAvg inference metrics}
    \label{tab:baseline_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        20 & 0.81 \\
        \bottomrule
    \end{tabular}
\end{table}

The baseline setup converges steadily despite the faulty device, but validation and inference accuracy plateau around 81\%, indicating residual class imbalance and the effect of noisy updates. Rewards remain uniform because FedAvg treats all updates equally, even when the faulty device drifts from the consensus.

\section{Dynamic Sampling Evaluation}

\begin{table}[h!]
    \centering
    \caption{Training metrics with dynamic sampling}
    \label{tab:dynamic_sampling}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 3000 & 0.82 & 0.41 & 0.79 & 0.44 & 6 \\
        0 & Device 2 & 3000 & 0.81 & 0.42 & 0.78 & 0.45 & 6 \\
        0 & Device 3 (faulty) & 3000 & 0.75 & 0.50 & 0.73 & 0.52 & 4 \\
        1 & Device 1 & 3000 & 0.86 & 0.37 & 0.83 & 0.40 & 6 \\
        1 & Device 2 & 3000 & 0.85 & 0.38 & 0.82 & 0.41 & 6 \\
        1 & Device 3 (faulty) & 3000 & 0.78 & 0.47 & 0.75 & 0.49 & 4 \\
        2 & Device 1 & 3000 & 0.88 & 0.34 & 0.85 & 0.38 & 6 \\
        2 & Device 2 & 3000 & 0.87 & 0.35 & 0.84 & 0.39 & 6 \\
        2 & Device 3 (faulty) & 3000 & 0.80 & 0.45 & 0.77 & 0.48 & 4 \\
        3 & Device 1 & 3000 & 0.89 & 0.32 & 0.86 & 0.36 & 6 \\
        3 & Device 2 & 3000 & 0.88 & 0.33 & 0.85 & 0.37 & 6 \\
        3 & Device 3 (faulty) & 3000 & 0.81 & 0.44 & 0.78 & 0.46 & 4 \\
        4 & Device 1 & 3000 & 0.90 & 0.30 & 0.87 & 0.34 & 6 \\
        4 & Device 2 & 3000 & 0.89 & 0.31 & 0.86 & 0.35 & 6 \\
        4 & Device 3 (faulty) & 3000 & 0.82 & 0.42 & 0.79 & 0.45 & 4 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with dynamic sampling}
    \label{tab:dynamic_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        20 & 0.85 \\
        \bottomrule
    \end{tabular}
\end{table}

Dynamic sampling improves validation and inference accuracy by roughly four percentage points. Rewards begin to diverge because honest devices supply more balanced updates, while the faulty device receives proportionally fewer tokens.

\section{With FedAvg\,+ }

\begin{table}[h!]
    \centering
    \caption{Training metrics with FedAvg\,+}
    \label{tab:fedavg_plus}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 2 & 8144 & 0.9853 & 0.0546 & 0.9902 & 0.0425 & 100.00 \\
        0 & Device 1 & 12216 & 0.9887 & 0.0463 & 0.9881 & 0.0387 & 60.00 \\
        0 & Device 3 (faulty) & 8142 & 0.8001 & 0.5063 & 0.7993 & 0.5058 & 28.57 \\
        1 & Device 2 & 8144 & 0.9851 & 0.0562 & 0.9902 & 0.0433 & 28.57 \\
        1 & Device 3 (faulty) & 8142 & 0.8001 & 0.5049 & 0.7993 & 0.5069 & 9.52 \\
        1 & Device 1 & 12216 & 0.9884 & 0.0491 & 0.9881 & 0.0399 & 42.86 \\
        2 & Device 2 & 8144 & 0.9876 & 0.0511 & 0.9902 & 0.0409 & 9.52 \\
        2 & Device 3 (faulty) & 8142 & 0.7995 & 0.5094 & 0.7993 & 0.5192 & 0.00 \\
        2 & Device 1 & 12216 & 0.9889 & 0.0442 & 0.9885 & 0.0374 & 21.43 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with FedAvg\,+}
    \label{tab:fedavg_plus_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 1.00 \\
        \bottomrule
    \end{tabular}
\end{table>

Waiting for all registered devices before aggregating, coupled with Krum-based selection, further suppresses the influence of the faulty device. Reward tokens now reflect contribution quality: honest devices receive higher payouts as their updates align with the consensus, while the faulty device's rewards diminish over successive rounds.

\section{Incremental Streaming Performance}

\begin{table}[h!]
    \centering
    \caption{Training metrics with streaming chunks}
    \label{tab:streaming_chunks}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 2000 & 0.81 & 0.42 & 0.78 & 0.45 & 6 \\
        0 & Device 2 & 2000 & 0.80 & 0.43 & 0.77 & 0.46 & 6 \\
        0 & Device 3 (faulty) & 2000 & 0.74 & 0.49 & 0.72 & 0.52 & 4 \\
        1 & Device 1 & 2000 & 0.85 & 0.38 & 0.82 & 0.41 & 6 \\
        1 & Device 2 & 2000 & 0.84 & 0.39 & 0.81 & 0.42 & 6 \\
        1 & Device 3 (faulty) & 2000 & 0.77 & 0.47 & 0.74 & 0.50 & 4 \\
        2 & Device 1 & 2000 & 0.87 & 0.35 & 0.84 & 0.38 & 7 \\
        2 & Device 2 & 2000 & 0.86 & 0.36 & 0.83 & 0.39 & 7 \\
        2 & Device 3 (faulty) & 2000 & 0.79 & 0.45 & 0.76 & 0.48 & 3 \\
        3 & Device 1 & 2000 & 0.88 & 0.33 & 0.85 & 0.36 & 7 \\
        3 & Device 2 & 2000 & 0.87 & 0.34 & 0.84 & 0.37 & 7 \\
        3 & Device 3 (faulty) & 2000 & 0.80 & 0.44 & 0.77 & 0.47 & 3 \\
        4 & Device 1 & 2000 & 0.89 & 0.31 & 0.86 & 0.34 & 7 \\
        4 & Device 2 & 2000 & 0.88 & 0.32 & 0.85 & 0.35 & 7 \\
        4 & Device 3 (faulty) & 2000 & 0.81 & 0.43 & 0.78 & 0.46 & 3 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with streaming}
    \label{tab:streaming_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        20 & 0.85 \\
        \bottomrule
    \end{tabular}
\end{table}

Streaming chunks accelerate convergence by ensuring each round consumes fresh data. Training and validation metrics track closely, and rewards remain skewed toward honest devices because their updates stay consistent across chunks.

\section{Discussion}

Across the scenarios, dynamic sampling and streaming deliver incremental gains, but their combination with FedAvg\,+ yields the most resilient training cycle. The Krum-based selection guards against corrupted updates, while device registration ensures fairness and transparent rewards. Blockchain logging distinguishes honest contributions from faulty ones, paving the way for incentive-driven federated deployments. These findings motivate the future work outlined in Chapter~\ref{chap:environment}.

