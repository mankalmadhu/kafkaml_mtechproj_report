\chapter{Results and Analysis}
\label{chap:results}

\section{Introduction}

This chapter consolidates the outcomes of the federated occupancy experiments executed with the automation pipeline and environment described earlier. Every run used three registered devices (two honest, one faulty) so the aggregator waited for all contributions before completing a round. Inference was performed once per experiment on the final aggregated model. We analyse four scenarios: baseline FedAvg, FedAvg with dynamic sampling, FedAvg\,+ (Krum), and incremental streaming. Blockchain logging was enabled in all runs so rewards could be audited.

\section{Metrics and Reporting Methodology}

Training and validation metrics (loss, accuracy) are sourced from the Kafka-ML backend for each aggregation round. Because all devices are registered, the training tables report per-device metrics per round (samples processed, accuracy, loss, reward tokens earned). Inference metrics are computed once per scenario from the predictions generated during the automation pipeline’s inference phase. Tables in this chapter use the following notation:

\begin{itemize}
    \item \textbf{Round} – Aggregation round index (0-based).
    \item \textbf{Samples} – Training samples processed by the device in that round.
    \item \textbf{Train Acc. / Loss} – Final training accuracy and loss.
    \item \textbf{Val. Acc. / Loss} – Validation accuracy and loss reported by the backend.
    \item \textbf{Reward} – Tokens credited to the device’s blockchain wallet in that round.
    \item \textbf{Inference Samples / Accuracy} – Number of inference samples and the resulting accuracy for the full experiment.
\end{itemize}

Unless noted otherwise, results are averaged across three consecutive runs to smooth transient variations.

\section{Baseline: FedAvg without Enhancements}

\begin{table}[h!]
    \centering
    \caption{Baseline FedAvg training metrics (registered devices)}
    \label{tab:baseline_fedavg}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 12216 & 0.9887 & 0.0463 & 0.9881 & 0.0387 & 60.00 \\
        0 & Device 2 & 8144 & 0.9853 & 0.0546 & 0.9902 & 0.0425 & 100.00 \\
        0 & Device 3 (faulty) & 8142 & 0.8001 & 0.5063 & 0.7993 & 0.5058 & 28.57 \\
        1 & Device 1 & 12216 & 0.9884 & 0.0491 & 0.9881 & 0.0399 & 42.86 \\
        1 & Device 2 & 8144 & 0.9851 & 0.0562 & 0.9902 & 0.0433 & 28.57 \\
        1 & Device 3 (faulty) & 8142 & 0.8001 & 0.5049 & 0.7993 & 0.5069 & 9.52 \\
        2 & Device 1 & 12216 & 0.9889 & 0.0442 & 0.9885 & 0.0374 & 21.43 \\
        2 & Device 2 & 8144 & 0.9876 & 0.0511 & 0.9902 & 0.0409 & 9.52 \\
        2 & Device 3 (faulty) & 8142 & 0.7995 & 0.5094 & 0.7993 & 0.5192 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Baseline FedAvg inference metrics}
    \label{tab:baseline_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 1.00 \\
        \bottomrule
    \end{tabular}
\end{table}

The baseline setup achieves stable convergence despite the faulty device. Honest devices reach high accuracy (0.98–0.99), while Device 3 remains lower at 0.80. Rewards diverge significantly across rounds (100.00 → 9.52 for Device 2), reflecting quality-based compensation. Inference accuracy is 100\%, demonstrating effective aggregation of honest contributions.

\section{Dynamic Sampling Evaluation}

\begin{table}[h!]
    \centering
    \caption{Training metrics with dynamic sampling}
    \label{tab:dynamic_sampling}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 12216 & 0.9914 & 0.0454 & 0.9915 & 0.0392 & 60.00 \\
        0 & Device 2 & 8144 & 0.9884 & 0.0575 & 0.9906 & 0.0454 & 100.00 \\
        0 & Device 3 (faulty) & 8142 & 0.5089 & 0.6927 & 0.5152 & 0.6930 & 28.57 \\
        1 & Device 1 & 12216 & 0.9901 & 0.0458 & 0.9912 & 0.0390 & 42.86 \\
        1 & Device 2 & 8144 & 0.9892 & 0.0546 & 0.9904 & 0.0455 & 28.57 \\
        1 & Device 3 (faulty) & 8142 & 0.5077 & 0.6924 & 0.5203 & 0.6920 & 14.28 \\
        2 & Device 1 & 12216 & 0.9917 & 0.0440 & 0.9921 & 0.0363 & 21.43 \\
        2 & Device 2 & 8144 & 0.9899 & 0.0510 & 0.9908 & 0.0433 & 9.52 \\
        2 & Device 3 (faulty) & 8142 & 0.5029 & 0.7010 & 0.5160 & 0.7026 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with dynamic sampling}
    \label{tab:dynamic_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 0.50 \\
        \bottomrule
    \end{tabular}
\end{table}

Dynamic sampling exacerbates the faulty device's negative impact. Device 3 produces random predictions (accuracy 0.50–0.52) and is penalized with declining rewards (100.00 → 28.57 → 0.00). Honest devices train well (accuracy 0.98–0.99), but the random updates degrade inference to 0.50, worse than the baseline.

\section{With FedAvg\,+ }

\begin{table}[h!]
    \centering
    \caption{Training metrics with FedAvg\,+}
    \label{tab:fedavg_plus}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 12216 & 0.9885 & 0.0476 & 0.9892 & 0.0417 & 60.00 \\
        0 & Device 2 & 8144 & 0.9862 & 0.0553 & 0.9883 & 0.0468 & 100.00 \\
        0 & Device 3 (faulty) & 8142 & 0.8001 & 0.5093 & 0.7999 & 0.5054 & 28.57 \\
        1 & Device 1 & 12216 & 0.9887 & 0.0489 & 0.9892 & 0.0428 & 42.86 \\
        1 & Device 2 & 8144 & 0.9859 & 0.0572 & 0.9884 & 0.0484 & 28.57 \\
        1 & Device 3 (faulty) & 8142 & 0.8001 & 0.5040 & 0.7999 & 0.5033 & 14.28 \\
        2 & Device 1 & 12216 & 0.9899 & 0.0440 & 0.9895 & 0.0404 & 21.43 \\
        2 & Device 2 & 8144 & 0.9873 & 0.0527 & 0.9885 & 0.0462 & 9.52 \\
        2 & Device 3 (faulty) & 8142 & 0.8001 & 0.5059 & 0.7999 & 0.5070 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with FedAvg\,+}
    \label{tab:fedavg_plus_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 1.00 \\
        \bottomrule
    \end{tabular}
\end{table}

FedAvg\,+ with Krum-based aggregation successfully mitigates the faulty device's influence. Honest devices achieve accuracy 0.98–0.99; Device 3 remains at 0.80. Krum’s outlier suppression plus declining rewards (28.57 → 14.28 → 0.00) reduce its weight; inference accuracy is 100\%.

\section{With Incremental Streaming}

\begin{table}[h!]
    \centering
    \caption{Training metrics with streaming chunks}
    \label{tab:streaming_chunks}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 4512 & 0.9882 & 0.0465 & 0.9895 & 0.0402 & 60.00 \\
        0 & Device 2 & 2314 & 0.9853 & 0.0548 & 0.9882 & 0.0461 & 100.00 \\
        0 & Device 3 (faulty) & 2308 & 0.8001 & 0.5058 & 0.7999 & 0.5041 & 28.57 \\
        1 & Device 1 & 4484 & 0.9887 & 0.0481 & 0.9891 & 0.0425 & 42.86 \\
        1 & Device 2 & 2284 & 0.9860 & 0.0576 & 0.9882 & 0.0479 & 28.57 \\
        1 & Device 3 (faulty) & 2338 & 0.8001 & 0.5040 & 0.7999 & 0.5019 & 14.28 \\
        2 & Device 1 & 4378 & 0.9895 & 0.0432 & 0.9895 & 0.0398 & 21.43 \\
        2 & Device 2 & 2364 & 0.9863 & 0.0524 & 0.9885 & 0.0458 & 9.52 \\
        2 & Device 3 (faulty) & 2317 & 0.7998 & 0.5103 & 0.7999 & 0.5057 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with streaming}
    \label{tab:streaming_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 1.00 \\
        \bottomrule
    \end{tabular}
\end{table}

With streaming data chunks, each round trains on fresh samples (4512 → 4484 → 4378 for Device 1). Honest devices maintain high accuracy (0.98–0.99) across rounds; Device 3 stays around 0.80. Rewards for Device 3 drop to zero by round 2; inference accuracy is 100\%.

\section{With Dynamic Sampling, FedAvg\,+, and Incremental Streaming}

\begin{table}[h!]
    \centering
    \caption{Training metrics with all enhancements combined}
    \label{tab:combined_enhancements}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \textbf{Round} & \textbf{Device} & \textbf{Samples} & \textbf{Train Acc.} & \textbf{Train Loss} & \textbf{Val. Acc.} & \textbf{Val. Loss} & \textbf{Reward} \\
        \midrule
        0 & Device 1 & 4153 & 0.9906 & 0.0459 & 0.9913 & 0.0347 & 60.00 \\
        0 & Device 2 & 2725 & 0.9888 & 0.0557 & 0.9918 & 0.0420 & 100.00 \\
        0 & Device 3 (faulty) & 2834 & 0.5229 & 0.6927 & 0.5005 & 0.6948 & 28.57 \\
        1 & Device 1 & 4034 & 0.9902 & 0.0476 & 0.9910 & 0.0355 & 42.86 \\
        1 & Device 2 & 2684 & 0.9888 & 0.0525 & 0.9910 & 0.0433 & 28.57 \\
        1 & Device 3 (faulty) & 2725 & 0.5172 & 0.6922 & 0.4950 & 0.6950 & 9.52 \\
        2 & Device 1 & 4112 & 0.9915 & 0.0424 & 0.9916 & 0.0340 & 21.43 \\
        2 & Device 2 & 2812 & 0.9901 & 0.0506 & 0.9918 & 0.0414 & 9.52 \\
        2 & Device 3 (faulty) & 2675 & 0.4923 & 0.7007 & 0.4843 & 0.7588 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Inference metrics with all enhancements}
    \label{tab:combined_inference}
    \begin{tabular}{c c}
        \toprule
        \textbf{Inference Samples} & \textbf{Accuracy} \\
        \midrule
        30 & 1.00 \\
        \bottomrule
    \end{tabular}
\end{table}

The combined setup (dynamic sampling, FedAvg+, and streaming) trains on fresh chunks per round, emphasizes high-quality contributions via Krum, and penalizes outliers. Device 3 yields near-random predictions (~0.49–0.52); its rewards drop to 0.00. Honest devices maintain high accuracy (0.98–0.99). Despite the fault, inference accuracy is 100\%, showing the combination’s robustness.

\section{Discussion}

Across the five scenarios, several findings emerge. The baseline configuration achieves 100\% inference despite the faulty device's lower accuracy (0.80). Adding dynamic sampling alone degrades inference to 0.50 because Device 3 produces random predictions; Krum-based aggregation and streaming each independently restore inference to 100\%. The combined setup (dynamic sampling, FedAvg\,+, and streaming) reaches 100\% accuracy despite Device 3’s random outputs, confirming the resilience of Krum-based outlier suppression. In all schemes with reward divergence, Device 3’s tokens fall to zero by round 2, showing quality-based compensation. These results validate device registration and blockchain-mediated rewards, paving the way for incentive-driven federated deployments. These findings motivate the future work outlined in Chapter~\ref{chap:environment}.

