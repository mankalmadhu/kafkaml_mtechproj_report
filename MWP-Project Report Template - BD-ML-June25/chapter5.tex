\chapter{Kafka-ML Platform Enhancements}

\section{Overview}

This chapter documents the key enhancements introduced during the project to extend Kafka-ML beyond the baseline described in Chapter~\ref{chap:kafkaml_setup}. The work focused on strengthening the automation, fairness, and robustness of the federated learning pipeline that powers smart building occupancy prediction. The contributions are organised into four thematic areas:

\begin{itemize}
    \item \textbf{End-to-End Automation Layer:} A reusable orchestration framework that can provision Kafka-ML components, drive experiments, and capture artefacts without manual intervention.
    \item \textbf{Occupancy Dataset Integration:} Dataset-specific formatting, control messages, and configuration hooks that seamlessly plug the occupancy workload into the automation layer and federated services.
    \item \textbf{Dynamic Sampling for Federated Training:} Label-weight aware data injection and weighted loss handling in the federated training module to mitigate class imbalance and reward under-represented events.
    \item \textbf{Advanced Aggregation and Streaming Improvements:} Enhancements to the aggregation logic (FedAvg\,+ with device registration and Krum-inspired filtering) and incremental chunk processing so that federated rounds operate on streaming sensor data.
\end{itemize}

Each subsequent section explains the design rationale, code-level changes, and interactions with existing Kafka-ML components for these additions. Execution setup, metrics, and outcome analysis are delegated to Chapters~6 and~7.

\section{End-to-End Automation Layer}
\label{sec:automation_layer}

The end-to-end automation layer lives under the top-level `e2e\_scripts/` directory. At its core is `automate\_e2e.py`, a command-line entry point that loads a YAML scenario, instantiates the matching dataset handler, and hands execution to `PipelineRunner`. The runner drives a deterministic sequence of phases—environment preparation, Kubernetes deployment, backend submissions, controller triggers, data injection, and artefact capture—so complete Kafka-ML experiments can be executed repeatably with a single command.

\begin{table}[h!]
    \centering
    \caption{Key components in `e2e\_scripts/`}
    \label{tab:e2e_components}
    \begin{tabular}{p{0.28\textwidth} p{0.62\textwidth}}
        \toprule
        \textbf{Path} & \textbf{Responsibility} \\
        \midrule
        `automate\_e2e.py` & CLI entry point; parses YAML config, selects dataset handler, invokes `PipelineRunner`. \\
        `automate\_mnist_federated_e2e.py` & Legacy scenario script retained for compatibility; shares helpers with the generic runner. \\
        `check_kafka_topic.py` & Utility for validating Kafka topic offsets and payloads during automation runs. \\
        `configs/` & YAML definitions describing datasets, deployment knobs, aggregation parameters, and optional step skips. \\
        `datasets/` & Dataset adapters (e.g., MNIST, occupancy) that expose common hooks for preparation and inference validation. \\
        `lib/` & Shared helpers for Kubernetes operations, Kafka interactions, blockchain contract clients, and the `PipelineRunner` implementation. \\
        `automation_results.json`, `automation_*.log` & Structured results and timestamped logs emitted after each run. \\
        `requirements_automation.txt` & Python dependencies required to execute the automation pipeline. \\
    \bottomrule
    \end{tabular}
\end{table}

This modular layout keeps orchestration logic decoupled from workload specifics, allowing subsequent enhancements (Sections~\ref{sec:occupancy_integration}--\ref{sec:incremental_federated}) to extend the pipeline without duplicating setup boilerplate.

