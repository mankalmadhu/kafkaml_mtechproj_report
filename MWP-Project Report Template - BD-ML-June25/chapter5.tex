\chapter{Kafka-ML Platform Enhancements}

\section{Overview}

This chapter documents the key enhancements introduced during the project to extend Kafka-ML beyond the baseline described in Chapter~\ref{chap:kafkaml_setup}. The work focused on strengthening the automation, fairness, and robustness of the federated learning pipeline that powers smart building occupancy prediction. The contributions are organised into four thematic areas:

\begin{itemize}
    \item \textbf{End-to-End Automation Layer:} A reusable orchestration framework that can provision Kafka-ML components, drive experiments, and capture artefacts without manual intervention.
    \item \textbf{Occupancy Dataset Integration:} Dataset-specific formatting, control messages, and configuration hooks that seamlessly plug the occupancy workload into the automation layer and federated services.
    \item \textbf{Dynamic Sampling for Federated Training:} Label-weight aware data injection and weighted loss handling in the federated training module to mitigate class imbalance and reward under-represented events.
    \item \textbf{Advanced Aggregation Logic:} A FedAvg\,+ workflow that incorporates device registration and Krum-inspired filtering before promoting global models.
    \item \textbf{Incremental Streaming Support:} Chunk-based scheduling and worker extensions that allow federated rounds to train on streaming sensor data without re-processing entire datasets.
\end{itemize}

Each subsequent section explains the design rationale, code-level changes, and interactions with existing Kafka-ML components for these additions. Execution setup, metrics, and outcome analysis are delegated to Chapters~6 and~7.

\section{End-to-End Automation Layer}
\label{sec:automation_layer}

The end-to-end automation layer lives under the top-level `e2e\_scripts/` directory. At its core is `automate\_e2e.py`, a command-line entry point that loads a YAML scenario, instantiates the matching dataset handler, and hands execution to `PipelineRunner`. The runner drives a deterministic sequence of phases—environment preparation, Kubernetes deployment, backend submissions, controller triggers, data injection, and artefact capture—so complete Kafka-ML experiments can be executed repeatably with a single command.

\noindent The pipeline follows seven repeatable steps:
\begin{enumerate}
    \item \textbf{Create Model} – Upload the TensorFlow architecture and metadata to the backend.
    \item \textbf{Create Configuration} – Bind training parameters, aggregation strategy, and dataset settings to the model.
    \item \textbf{Create Deployment} – Launch the Kubernetes jobs (classic or federated) and generate a deployment ID plus federated string.
    \item \textbf{Inject Data} – Publish preprocessed samples (and control metadata) to Kafka topics via `FederatedRawSink`; handles single vs.~multi-device paths, optional dynamic sampling weights, and incremental streaming chunks.
    \item \textbf{Monitor Training} – Poll the backend until the training job reports completion and collect intermediate metrics.
    \item \textbf{Create Inference Engine} – Spin up an inference deployment wired to the trained model and inference topics.
    \item \textbf{Run Inference} – Stream test samples through Kafka and log predictions for downstream analysis.
\end{enumerate}

\begin{table}[h!]
    \centering
    \caption{Key components in `e2e\_scripts/`}
    \label{tab:e2e_components}
    \begin{tabular}{p{0.28\textwidth} p{0.62\textwidth}}
        \toprule
        \textbf{Path} & \textbf{Responsibility} \\
        \midrule
        `automate\_e2e.py` & CLI entry point; parses YAML config, selects dataset handler, invokes `PipelineRunner`. \\
        `configs/` & YAML definitions describing datasets, deployment knobs, aggregation parameters, and optional step skips. \\
        `datasets/` & Dataset adapters (e.g., MNIST, occupancy) that expose common hooks for preparation and inference validation. \\
        `lib/` & Shared helpers for Kubernetes operations, Kafka interactions, blockchain contract clients, and the `PipelineRunner` implementation. \\
        `automation_*.log` & Timestamped logs emitted after each run, complementing structured JSON summaries consumed in later chapters. \\
    \bottomrule
    \end{tabular}
\end{table}

This modular layout keeps orchestration logic decoupled from workload specifics, allowing subsequent enhancements (Sections~\ref{sec:occupancy_integration}--\ref{sec:incremental_federated}) to extend the pipeline without duplicating setup boilerplate.

\section{Occupancy Dataset Integration}
\label{sec:occupancy_integration}

To support smart-building experiments, the automation layer introduces a dedicated occupancy adapter under `datasets/`. The \texttt{OccupancyDataset} class extends the shared base dataset and provides:

\begin{itemize}
    \item deterministic loading of the raw CSV files for training, validation, and inference, with result caching to avoid repeated disk reads;
    \item feature normalisation through \texttt{StandardScaler}, keeping the relative dynamics of temperature, humidity, light, CO$_2$, and humidity ratio intact while producing TensorFlow-ready tensors; and
    \item helper utilities such as \texttt{compute\_label\_weights\_from\_data()} and \texttt{get\_class\_distribution\_from\_data()} that enable downstream features like dynamic sampling.
\end{itemize}

The adapter exposes a uniform interface\textemdash\texttt{load\_training\_data()}, \texttt{load\_test\_data()}, \texttt{load\_inference\_data()}, and \texttt{parse\_prediction()}\textemdash so \texttt{PipelineRunner} can switch between MNIST and occupancy scenarios without altering orchestration logic.

During data injection, \texttt{PipelineRunner} retrieves the preprocessed tensors and streams them into Kafka via \texttt{FederatedRawSink}. The single-device path (\texttt{\_inject\_training\_data\_single\_device}) reads the configured sample counts, ensures the topics exist through \texttt{KafkaAdmin}, and calls \texttt{send()} for each record. The multi-device path iterates over the registered devices declared in the YAML, loads the device-specific files (including optional `faulty` datasets), and attaches label weights when dynamic sampling is enabled. Once all records are published, the sink emits a control message whose \texttt{topic} field encodes the partition and offset range that contain the uploaded samples, allowing the federated backend to detect collisions and dispatch worker jobs correctly.

The sink layer was extended to make this workflow reliable. \texttt{FederatedRawSink} now accepts optional \texttt{label\_weights} and \texttt{streaming\_data\_chunks} parameters and inherits improved bookkeeping from \texttt{KafkaMLSink}. Each publish stores record metadata so the control message accurately reports offsets, message counts, and streaming chunk settings. Consequently, federated controllers receive both the data-control envelope and the precise coordinates of the sensor samples, while automation logs continue to track per-device statistics that feed into the evaluation presented in Chapter~6.

\section{Dynamic Sampling for Federated Training}
\label{sec:dynamic_sampling}

To address class imbalance in occupancy data (where ``vacant'' readings dominate during off hours), the automation pipeline computes device-specific label weights and passes them through to the federated training service. During multi-device data injection, \texttt{PipelineRunner} inspects each device’s labels by calling \texttt{dataset.compute\_label\_weights\_from\_data()}. The helper returns inverse-frequency weights which the runner stores alongside the per-device configuration. These weights are serialised into the data-control message so downstream components can rehydrate them when the worker job starts.

On the training side, \texttt{federated\_mainTraining.py} consumes the label-weight metadata and conditionally applies dynamic sampling. When \texttt{LABEL\_WEIGHTS} is present in the environment, the dataset maps are wrapped with \texttt{assign\_sample\_weight()}, ensuring each mini-batch carries a third element (features, label, weight). TensorFlow’s training loop then incorporates these weights when computing gradients, so under-represented occupancy events receive proportionally higher influence during model updates. The same weighting is applied to validation splits, keeping evaluation metrics consistent with the sampling regime.

\section{FedAvg\,+ with Registered Devices}
\label{sec:fedavg_plus}

To strengthen robustness against stragglers or unreliable updates, the automation framework now registers devices explicitly during the deployment phase. \texttt{PipelineRunner} counts the number of devices declared in the data-injection configuration and passes that figure as the \texttt{registered\_devices} argument when invoking the backend’s deployment API. This count flows through Kafka-ML and informs the aggregation service how many client models must arrive before a round can be finalised.

The edge aggregation logic, implemented in \texttt{edgeBlockchainBasedTraining.py}, enforces this contract. Before aggregating, the training loop verifies that the number of collected client models matches \texttt{registered\_devices}; if not, it waits while logging progress (lines~195--205). Once the threshold is met (lines~207--211), the service proceeds with aggregation and resets its queue for the next round. This behaviour guarantees that every registered participant contributes to each FedAvg\,+ iteration, preventing partial updates from influencing the global model.

Aggregation is delegated to the new \texttt{aggregation\_strategies.py} module. Alongside standard FedAvg, the module introduces \texttt{KrumStrategy}, which selects the model closest to the consensus of all submissions, and \texttt{FedAvgPlusStrategy}, which currently wraps Krum but can be extended with weighted averaging. By choosing FedAvg\,+ in the deployment configuration, the automation pipeline switches the controller to the Krum-based selection logic after all devices report, combining strict device coordination with outlier-resistant aggregation.

