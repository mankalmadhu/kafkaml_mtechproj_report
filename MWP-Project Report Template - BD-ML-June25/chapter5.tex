\chapter{Kafka-ML Platform Enhancements}

\section{Overview}

This chapter documents the key enhancements introduced during the project to extend Kafka-ML beyond the baseline described in Chapter~\ref{chap:kafkaml_setup}. The work focused on strengthening the automation, fairness, and robustness of the federated learning pipeline that powers smart building occupancy prediction. The contributions are organised into four thematic areas:

\begin{itemize}
    \item \textbf{End-to-End Automation Layer:} A reusable orchestration framework that can provision Kafka-ML components, drive experiments, and capture artefacts without manual intervention.
    \item \textbf{Occupancy Dataset Integration:} Dataset-specific formatting, control messages, and configuration hooks that seamlessly plug the occupancy workload into the automation layer and federated services.
    \item \textbf{Dynamic Sampling for Federated Training:} Label-weight aware data injection and weighted loss handling in the federated training module to mitigate class imbalance and reward under-represented events.
    \item \textbf{Advanced Aggregation Logic:} A FedAvg\,+ workflow that incorporates device registration and Krum-inspired filtering before promoting global models.
    \item \textbf{Incremental Streaming Support:} Chunk-based scheduling and worker extensions that allow federated rounds to train on streaming sensor data without re-processing entire datasets.
\end{itemize}

Each subsequent section explains the design rationale, code-level changes, and interactions with existing Kafka-ML components for these additions. Execution setup, metrics, and outcome analysis are delegated to Chapters~6 and~7.

\section{End-to-End Automation Layer}
\label{sec:automation_layer}

The end-to-end automation layer lives under the top-level `e2e\_scripts/` directory. At its core is `automate\_e2e.py`, a command-line entry point that loads a YAML scenario, instantiates the matching dataset handler, and hands execution to `PipelineRunner`. The runner drives a deterministic sequence of phases—environment preparation, Kubernetes deployment, backend submissions, controller triggers, data injection, and artefact capture—so complete Kafka-ML experiments can be executed repeatably with a single command.

\begin{table}[h!]
    \centering
    \caption{Key components in `e2e\_scripts/`}
    \label{tab:e2e_components}
    \begin{tabular}{p{0.28\textwidth} p{0.62\textwidth}}
        \toprule
        \textbf{Path} & \textbf{Responsibility} \\
        \midrule
        `automate\_e2e.py` & CLI entry point; parses YAML config, selects dataset handler, invokes `PipelineRunner`. \\
        `configs/` & YAML definitions describing datasets, deployment knobs, aggregation parameters, and optional step skips. \\
        `datasets/` & Dataset adapters (e.g., MNIST, occupancy) that expose common hooks for preparation and inference validation. \\
        `lib/` & Shared helpers for Kubernetes operations, Kafka interactions, blockchain contract clients, and the `PipelineRunner` implementation. \\
        `automation_*.log` & Timestamped logs emitted after each run, complementing structured JSON summaries consumed in later chapters. \\
    \bottomrule
    \end{tabular}
\end{table}

This modular layout keeps orchestration logic decoupled from workload specifics, allowing subsequent enhancements (Sections~\ref{sec:occupancy_integration}--\ref{sec:incremental_federated}) to extend the pipeline without duplicating setup boilerplate.

\section{Occupancy Dataset Integration}
\label{sec:occupancy_integration}

To support smart-building experiments, the automation layer introduces a dedicated occupancy adapter under `datasets/`. The \texttt{OccupancyDataset} class extends the shared base dataset and provides:

\begin{itemize}
    \item deterministic loading of the raw CSV files for training, validation, and inference, with result caching to avoid repeated disk reads;
    \item feature normalisation through \texttt{StandardScaler}, keeping the relative dynamics of temperature, humidity, light, CO$_2$, and humidity ratio intact while producing TensorFlow-ready tensors; and
    \item helper utilities such as \texttt{compute\_label\_weights\_from\_data()} and \texttt{get\_class\_distribution\_from\_data()} that enable downstream features like dynamic sampling.
\end{itemize}

The adapter exposes a uniform interface\textemdash\texttt{load\_training\_data()}, \texttt{load\_test\_data()}, \texttt{load\_inference\_data()}, and \texttt{parse\_prediction()}\textemdash so \texttt{PipelineRunner} can switch between MNIST and occupancy scenarios without altering orchestration logic.

During data injection, \texttt{PipelineRunner} retrieves the preprocessed tensors and streams them into Kafka via \texttt{FederatedRawSink}. The single-device path (\texttt{\_inject\_training\_data\_single\_device}) reads the configured sample counts, ensures the topics exist through \texttt{KafkaAdmin}, and calls \texttt{send()} for each record. The multi-device path iterates over the registered devices declared in the YAML, loads the device-specific files (including optional `faulty` datasets), and attaches label weights when dynamic sampling is enabled. Once all records are published, the sink emits a control message whose \texttt{topic} field encodes the partition and offset range that contain the uploaded samples, allowing the federated backend to detect collisions and dispatch worker jobs correctly.

The sink layer was extended to make this workflow reliable. \texttt{FederatedRawSink} now accepts optional \texttt{label\_weights} and \texttt{streaming\_data\_chunks} parameters and inherits improved bookkeeping from \texttt{KafkaMLSink}. Each publish stores record metadata so the control message accurately reports offsets, message counts, and streaming chunk settings. Consequently, federated controllers receive both the data-control envelope and the precise coordinates of the sensor samples, while automation logs continue to track per-device statistics that feed into the evaluation presented in Chapter~6.

\section{Dynamic Sampling for Federated Training}
\label{sec:dynamic_sampling}

To address class imbalance in occupancy data (where ``vacant'' readings dominate during off hours), the automation pipeline computes device-specific label weights and passes them through to the federated training service. During multi-device data injection, \texttt{PipelineRunner} inspects each device’s labels by calling \texttt{dataset.compute\_label\_weights\_from\_data()}. The helper returns inverse-frequency weights which the runner stores alongside the per-device configuration. These weights are serialised into the data-control message so downstream components can rehydrate them when the worker job starts.

On the training side, \texttt{federated\_mainTraining.py} consumes the label-weight metadata and conditionally applies dynamic sampling. When \texttt{LABEL\_WEIGHTS} is present in the environment, the dataset maps are wrapped with \texttt{assign\_sample\_weight()}, ensuring each mini-batch carries a third element (features, label, weight). TensorFlow’s training loop then incorporates these weights when computing gradients, so under-represented occupancy events receive proportionally higher influence during model updates. The same weighting is applied to validation splits, keeping evaluation metrics consistent with the sampling regime.

