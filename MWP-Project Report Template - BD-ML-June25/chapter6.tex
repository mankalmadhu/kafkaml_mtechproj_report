\chapter{Experiment Environment}
\label{chap:environment}

\section{Overview}

This chapter documents the environment used to generate the results in Chapter~\ref{chap:results}. Every experiment was orchestrated through the automation pipeline described previously, but reproducibility hinges on the concrete configuration, dataset partitions, and deployment overlays captured here. The objective is to provide a precise runbook so that reviewers can recreate the three-device occupancy scenario (including a deliberately faulty client) on demand.

\section{Scenario Configuration}
\label{sec:scenario_config}

The entry point for the experiments is the YAML file \texttt{configs/occupancy\_federated\_3device\_faulty.yaml}. Its key sections are summarised below:

\begin{itemize}
    \item \textbf{Model}: defines a TensorFlow binary-classification network with three dense layers and dropout, compiled with Adam and weighted metrics.
    \item \textbf{Configuration}: binds the model to the federated training workflow and provides descriptive metadata for traceability.
    \item \textbf{Deployment}: enables federated training with blockchain logging, sets five aggregation rounds, specifies the FedAvg aggregation strategy (extended to FedAvg\,+ when configured), and declares minimum sample requirements per round.
\item \textbf{Data Injection}: lists common Kafka parameters, toggles streaming chunk support, and outlines device-specific topics, control channels, and dataset files (including one faulty source).
    \item \textbf{Inference}: configures the inference deployment, including the RAW input format, Kafka topics, and the number of predictions captured after each run.
\end{itemize}

Together these sections fully describe the payloads passed to \texttt{automate\_e2e.py} when launching experiments.

\section{Dataset Partitioning and Fault Injection}
\label{sec:dataset_partitioning}

Each federated device consumes a dedicated dataset stored under \texttt{datasets/}:

\begin{itemize}
    \item \texttt{device1\_data.txt} and \texttt{device2\_data.txt} contain clean occupancy readings drawn from the canonical training set, providing approximately 3{,}000 samples per device after preprocessing.
    \item \texttt{datatraining\_faulty.txt} injects corrupted or biased readings to emulate a misbehaving edge node; the automation script flags this device via \texttt{faulty\_device: true} in the YAML.
\end{itemize}

The automation layer loads these files through \texttt{OccupancyDataset}, normalises the features, and (optionally) computes per-device label weights. Validation rates (20\%) are applied consistently so the backend can split training and validation subsets deterministically. The faulty device is crucial for evaluating robustness of dynamic sampling and FedAvg\,+ in the subsequent results chapter.

\section{Per-Device Deployment Layout}
\label{sec:deployment_layout}

Device-specific Kubernetes overlays reside under \texttt{federated-module/kustomize/dev1}, \texttt{dev2}, and \texttt{dev3}. Each overlay targets its own Kubernetes namespace (for example, \texttt{kafkamldevice1}, \texttt{kafkamldevice2}, and \texttt{kafkamldevice3}), ensuring that controller jobs, workers, and supporting services remain isolated. Within each namespace, the overlay provisions:

\begin{itemize}
    \item federated backend and control loggers pointed at device-specific topics;
    \item controller and worker jobs that honour the registered device ID and associated control topics;
    \item blockchain helpers (smart-contract client and device-specific wallet secrets) when blockchain mode is enabled, allowing the controller to issue rewards directly to each registered deviceâ€™s account.
\end{itemize}

Although all devices share the same Kafka cluster and backend API, these namespace-bound overlays emulate geographically separated edges by isolating components, control channels, and blockchain accounts. The registered-device count derived from the YAML ensures the aggregation layer waits for all three deployments during each round before issuing on-chain rewards.

\section{Execution Runbook}
\label{sec:execution_runbook}


\section{Execution Runbook}
\label{sec:execution_runbook}

Experiments are launched with:

\begin{verbatim}
python automate_e2e.py --config configs/occupancy_federated_3device_faulty.yaml
\end{verbatim}

No skip flags were used in the baseline runs; automation logs (\texttt{automation\_*.log}) and structured summaries (\texttt{automation\_results.json}) were archived for analysis. Prior to executing the script, the Kafka-ML core and federated modules were deployed via:

\begin{verbatim}
kubectl apply -k kustomize/local
kubectl apply -k federated-module/kustomize/dev1
kubectl apply -k federated-module/kustomize/dev2
kubectl apply -k federated-module/kustomize/dev3
\end{verbatim}

The automation pipeline performed model/config/deployment creation, data injection, training monitoring, inference, and summary generation without manual intervention. All runs were executed on a local Kubernetes cluster (Minikube) with Kafka brokers accessible at `localhost:9094` and the backend API at `http://localhost:9090`.

\section{Summary}

This environment chapter captures the exact configuration, datasets, and deployment overlays underpinning the experiments. With these artefacts and commands, the results in Chapter~\ref{chap:results} can be replicated or extended with alternative aggregation strategies, dataset splits, or device counts.

